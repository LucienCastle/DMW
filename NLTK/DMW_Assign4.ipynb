{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newcorpus = [\"data mining is a good domain for research\",\"we are studying data mining today\",\"data mining has a promising career\"]\n",
    "vectorizer1 = TfidfVectorizer()\n",
    "vectorizer1.fit(newcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': 2, 'mining': 8, 'is': 7, 'good': 5, 'domain': 3, 'for': 4, 'research': 10, 'we': 13, 'are': 0, 'studying': 11, 'today': 12, 'has': 6, 'promising': 9, 'career': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.69314718 1.69314718 1.         1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718 1.         1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer1.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "newvector = vectorizer1.transform([newcorpus[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.52004008 0.30714405 0.         0.         0.\n",
      "  0.52004008 0.         0.30714405 0.52004008 0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(newvector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "We meet his boss, Ted Waller, a lover of puzzles. Waller fires Bryson from the Directorate, saying he's lost his touch; Bryson is now told to live as a professor of Byzantine history under the alias of Jonas Barett. After some initial drunkenness and a search for oblivion because his wife, Elena, has left him, he agrees to take the job. He lives under this alias for 5 years and becomes a popular professor, until the Deputy Director of Central Intelligence at the CIA, Harry Dunne, confronts him with a shocking revelation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"train\"\n",
    "files = []\n",
    "# fp = open(path+\"/\"+\"train.txt\",\"r\")\n",
    "for file in os.listdir(path):\n",
    "    try:\n",
    "        fp = open(path+\"/\"+file)\n",
    "        files.append(fp.read())\n",
    "    except:\n",
    "        fp.close()\n",
    "print(len(files))\n",
    "print(files[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'meet', 'boss', ',', 'Ted', 'Waller', ',', 'lover', 'puzzles', '.', 'Waller', 'fires', 'Bryson', 'Directorate', ',', 'saying', \"'s\", 'lost', 'touch', ';', 'Bryson', 'told', 'live', 'professor', 'Byzantine', 'history', 'alias', 'Jonas', 'Barett', '.', 'After', 'initial', 'drunkenness', 'search', 'oblivion', 'wife', ',', 'Elena', ',', 'left', ',', 'agrees', 'take', 'job', '.', 'He', 'lives', 'alias', '5', 'years', 'becomes', 'popular', 'professor', ',', 'Deputy', 'Director', 'Central', 'Intelligence', 'CIA', ',', 'Harry', 'Dunne', ',', 'confronts', 'shocking', 'revelation', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_files = []\n",
    "for file in files:\n",
    "    word_tokens = word_tokenize(file)\n",
    "    filtered_tokens = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_tokens.append(w)\n",
    "#     print(word_tokens)\n",
    "    filtered_files.append(filtered_tokens)\n",
    "print(filtered_files[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'meet', 'boss', ',', 'ted', 'waller', ',', 'lover', 'puzzl', '.', 'waller', 'fire', 'bryson', 'director', ',', 'say', \"'s\", 'lost', 'touch', ';', 'bryson', 'told', 'live', 'professor', 'byzantin', 'histori', 'alia', 'jona', 'barett', '.', 'after', 'initi', 'drunken', 'search', 'oblivion', 'wife', ',', 'elena', ',', 'left', ',', 'agre', 'take', 'job', '.', 'He', 'live', 'alia', '5', 'year', 'becom', 'popular', 'professor', ',', 'deputi', 'director', 'central', 'intellig', 'cia', ',', 'harri', 'dunn', ',', 'confront', 'shock', 'revel', '.']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_files = []\n",
    "for file in filtered_files:\n",
    "    stemmed_tokens = []\n",
    "    for w in file:\n",
    "        stemmed_tokens.append(ps.stem(w))\n",
    "    stemmed_files.append(stemmed_tokens)\n",
    "print(stemmed_files[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vctr = CountVectorizer()\n",
    "# print(files)\n",
    "vctr.fit(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'at': 20, 'another': 13, 'point': 152, 'when': 219, 'he': 89, 'is': 109, 'about': 2, 'to': 200, 'be': 24, 'shot': 175, 'by': 34, 'former': 78, 'enemy': 60, 'saved': 169, 'waller': 212, 'who': 221, 'explains': 71, 'that': 190, 'harry': 86, 'dunne': 57, 'really': 164, 'part': 149, 'of': 139, 'prometheus': 158, 'an': 11, 'organization': 143, 'business': 32, 'executives': 70, 'and': 12, 'powerful': 156, 'politicians': 154, 'around': 16, 'the': 191, 'world': 224, 'members': 132, 'are': 15, 'pushing': 161, 'treaty': 204, 'on': 140, 'surveillance': 183, 'which': 220, 'would': 225, 'allow': 8, 'for': 77, 'international': 108, 'super': 181, 'fbi': 73, 'their': 192, 'own': 147, 'richard': 166, 'lanchester': 116, 'businessman': 33, 'turned': 206, 'politician': 153, 'head': 90, 'implication': 100, 'this': 198, 'because': 25, 'its': 111, 'information': 104, 'companies': 39, 'then': 194, 'able': 1, 'monitor': 134, 'everything': 68, 'went': 217, 'in': 101, 'thereby': 196, 'control': 43, 'it': 110, 'story': 180, 'begins': 28, 'with': 223, 'protagonist': 159, 'under': 207, 'alias': 7, 'technician': 186, 'deep': 48, 'cover': 45, 'stop': 179, 'hezbollah': 93, 'terrorist': 188, 'from': 79, 'overthrowing': 146, 'government': 84, 'tunisia': 205, 'operation': 142, 'appears': 14, 'going': 83, 'well': 216, 'until': 209, 'terrorists': 189, 'discover': 54, 'weapons': 215, 'has': 87, 'supplied': 182, 'them': 193, 'defective': 49, 'before': 27, 'ensuing': 63, 'battle': 23, 'over': 145, 'though': 199, 'abu': 3, 'leader': 119, 'agency': 5, 'manages': 128, 'stab': 178, 'him': 94, 'abdomen': 0, 'helicoptered': 92, 'out': 144, 'we': 214, 'next': 136, 'find': 74, 'entering': 64, 'headquarters': 91, 'directorate': 53, 'learn': 120, 'russian': 168, 'intelligence': 106, 'created': 46, 'gru': 85, 'masterminds': 129, 'essentially': 66, 'penetration': 150, 'american': 10, 'soil': 176, 'learns': 121, 'his': 95, 'boss': 30, 'gennady': 80, 'rosovsky': 167, 'assumed': 19, 'name': 135, 'ted': 187, 'after': 4, 'english': 62, 'poet': 151, 'edmund': 58, 'says': 171, 'bryson': 31, 'entire': 65, 'life': 123, 'including': 102, 'parents': 148, 'death': 47, 'was': 213, 'engineered': 61, 'lead': 118, 'every': 67, 'mission': 133, 'undertaken': 208, 'designed': 51, 'hurt': 99, 'interests': 107, 'horrifies': 97, 'convinced': 44, 'go': 81, 'infiltrates': 103, 'tanker': 185, 'what': 218, 'they': 197, 're': 163, 'doing': 55, 'amassing': 9, 'there': 195, 'meets': 131, 'layla': 117, 'blowing': 29, 'up': 210, 'arsenal': 17, 'continues': 42, 'search': 172, 'however': 98, 'seems': 173, 'everywhere': 69, 'goes': 82, 'attack': 21, 'follows': 76, 'pursues': 160, 'trail': 203, 'contacts': 41, 'colleague': 38, 'jan': 112, 'vansina': 211, 'only': 141, 'have': 88, 'killed': 115, 'eyes': 72, 'meet': 130, 'lover': 127, 'puzzles': 162, 'fires': 75, 'saying': 170, 'lost': 126, 'touch': 202, 'now': 137, 'told': 201, 'live': 124, 'as': 18, 'professor': 157, 'byzantine': 35, 'history': 96, 'jonas': 114, 'barett': 22, 'some': 177, 'initial': 105, 'drunkenness': 56, 'oblivion': 138, 'wife': 222, 'elena': 59, 'left': 122, 'agrees': 6, 'take': 184, 'job': 113, 'lives': 125, 'years': 226, 'becomes': 26, 'popular': 155, 'deputy': 50, 'director': 52, 'central': 36, 'cia': 37, 'confronts': 40, 'shocking': 174, 'revelation': 165}\n"
     ]
    }
   ],
   "source": [
    "print(vctr.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "The Prometheus Deception is a spy fiction thriller novel written in 2000 by Robert Ludlum about an agent in an ultraclandestine agency known only as the Directorate named Nick Bryson, alias Jonas Barett, alias Jonathan Coleridge, alias The Technician, who is thrown into a fight between an organization he knows as Prometheus and his former employers at the Directorate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"test\"\n",
    "test_files = []\n",
    "# fp = open(path+\"/\"+\"train.txt\",\"r\")\n",
    "for file in os.listdir(path):\n",
    "    try:\n",
    "        fp = open(path+\"/\"+file)\n",
    "        test_files.append(fp.read())\n",
    "    except:\n",
    "        fp.close()\n",
    "print(len(test_files))\n",
    "print(test_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "newvector = vctr.transform(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 7 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 1 0 3 0 0 0 3 1 0 0 0 0 0 2 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0\n",
      "  0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(newvector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'at': 20, 'another': 13, 'point': 152, 'when': 219, 'he': 89, 'is': 109, 'about': 2, 'to': 200, 'be': 24, 'shot': 175, 'by': 34, 'former': 78, 'enemy': 60, 'saved': 169, 'waller': 212, 'who': 221, 'explains': 71, 'that': 190, 'harry': 86, 'dunne': 57, 'really': 164, 'part': 149, 'of': 139, 'prometheus': 158, 'an': 11, 'organization': 143, 'business': 32, 'executives': 70, 'and': 12, 'powerful': 156, 'politicians': 154, 'around': 16, 'the': 191, 'world': 224, 'members': 132, 'are': 15, 'pushing': 161, 'treaty': 204, 'on': 140, 'surveillance': 183, 'which': 220, 'would': 225, 'allow': 8, 'for': 77, 'international': 108, 'super': 181, 'fbi': 73, 'their': 192, 'own': 147, 'richard': 166, 'lanchester': 116, 'businessman': 33, 'turned': 206, 'politician': 153, 'head': 90, 'implication': 100, 'this': 198, 'because': 25, 'its': 111, 'information': 104, 'companies': 39, 'then': 194, 'able': 1, 'monitor': 134, 'everything': 68, 'went': 217, 'in': 101, 'thereby': 196, 'control': 43, 'it': 110, 'story': 180, 'begins': 28, 'with': 223, 'protagonist': 159, 'under': 207, 'alias': 7, 'technician': 186, 'deep': 48, 'cover': 45, 'stop': 179, 'hezbollah': 93, 'terrorist': 188, 'from': 79, 'overthrowing': 146, 'government': 84, 'tunisia': 205, 'operation': 142, 'appears': 14, 'going': 83, 'well': 216, 'until': 209, 'terrorists': 189, 'discover': 54, 'weapons': 215, 'has': 87, 'supplied': 182, 'them': 193, 'defective': 49, 'before': 27, 'ensuing': 63, 'battle': 23, 'over': 145, 'though': 199, 'abu': 3, 'leader': 119, 'agency': 5, 'manages': 128, 'stab': 178, 'him': 94, 'abdomen': 0, 'helicoptered': 92, 'out': 144, 'we': 214, 'next': 136, 'find': 74, 'entering': 64, 'headquarters': 91, 'directorate': 53, 'learn': 120, 'russian': 168, 'intelligence': 106, 'created': 46, 'gru': 85, 'masterminds': 129, 'essentially': 66, 'penetration': 150, 'american': 10, 'soil': 176, 'learns': 121, 'his': 95, 'boss': 30, 'gennady': 80, 'rosovsky': 167, 'assumed': 19, 'name': 135, 'ted': 187, 'after': 4, 'english': 62, 'poet': 151, 'edmund': 58, 'says': 171, 'bryson': 31, 'entire': 65, 'life': 123, 'including': 102, 'parents': 148, 'death': 47, 'was': 213, 'engineered': 61, 'lead': 118, 'every': 67, 'mission': 133, 'undertaken': 208, 'designed': 51, 'hurt': 99, 'interests': 107, 'horrifies': 97, 'convinced': 44, 'go': 81, 'infiltrates': 103, 'tanker': 185, 'what': 218, 'they': 197, 're': 163, 'doing': 55, 'amassing': 9, 'there': 195, 'meets': 131, 'layla': 117, 'blowing': 29, 'up': 210, 'arsenal': 17, 'continues': 42, 'search': 172, 'however': 98, 'seems': 173, 'everywhere': 69, 'goes': 82, 'attack': 21, 'follows': 76, 'pursues': 160, 'trail': 203, 'contacts': 41, 'colleague': 38, 'jan': 112, 'vansina': 211, 'only': 141, 'have': 88, 'killed': 115, 'eyes': 72, 'meet': 130, 'lover': 127, 'puzzles': 162, 'fires': 75, 'saying': 170, 'lost': 126, 'touch': 202, 'now': 137, 'told': 201, 'live': 124, 'as': 18, 'professor': 157, 'byzantine': 35, 'history': 96, 'jonas': 114, 'barett': 22, 'some': 177, 'initial': 105, 'drunkenness': 56, 'oblivion': 138, 'wife': 222, 'elena': 59, 'left': 122, 'agrees': 6, 'take': 184, 'job': 113, 'lives': 125, 'years': 226, 'becomes': 26, 'popular': 155, 'deputy': 50, 'director': 52, 'central': 36, 'cia': 37, 'confronts': 40, 'shocking': 174, 'revelation': 165}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.09861229 2.09861229 2.09861229 2.09861229 1.40546511 1.69314718\n",
      " 2.09861229 1.69314718 2.09861229 2.09861229 2.09861229 1.69314718\n",
      " 1.18232156 2.09861229 2.09861229 1.69314718 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 1.69314718 2.09861229 2.09861229 2.09861229\n",
      " 1.40546511 1.69314718 2.09861229 1.69314718 2.09861229 2.09861229\n",
      " 1.69314718 1.40546511 2.09861229 2.09861229 1.69314718 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 1.18232156\n",
      " 2.09861229 2.09861229 2.09861229 1.40546511 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 1.69314718 2.09861229 2.09861229 1.40546511\n",
      " 1.69314718 1.69314718 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 1.69314718 1.40546511 2.09861229 1.\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 1.40546511 1.40546511\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 1.69314718\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 1.69314718 2.09861229\n",
      " 2.09861229 1.         1.69314718 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 1.         1.69314718 2.09861229 1.69314718 1.69314718\n",
      " 1.69314718 2.09861229 2.09861229 2.09861229 2.09861229 1.69314718\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 1.69314718 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 1.69314718 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 1.69314718 1.69314718 2.09861229 1.18232156 1.\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 1.69314718 2.09861229 1.         2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 1.69314718 2.09861229 1.69314718\n",
      " 2.09861229 2.09861229 1.40546511 2.09861229 1.40546511 1.69314718\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 1.69314718 1.40546511\n",
      " 2.09861229 1.40546511 2.09861229 2.09861229 2.09861229]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.16751461 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.09437481 0.         0.         0.         0.         0.\n",
      "  0.16751461 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.4054492  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16751461 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.11218649\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.13514973 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.39910805 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33502922 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.16751461 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.13514973 0.         0.         0.55875127\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16751461 0.15964322 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16751461 0.         0.        ]\n",
      " [0.         0.         0.16163659 0.         0.         0.13040738\n",
      "  0.         0.39122215 0.         0.         0.         0.39122215\n",
      "  0.09106324 0.         0.         0.         0.         0.\n",
      "  0.32327318 0.         0.13040738 0.         0.16163659 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.10824991 0.         0.         0.13040738 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.18212647\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.13040738 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.0770207\n",
      "  0.         0.         0.         0.         0.         0.10824991\n",
      "  0.         0.         0.         0.         0.         0.26081477\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.1540414  0.         0.         0.         0.\n",
      "  0.16163659 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.16163659 0.         0.13040738\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.32327318 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16163659 0.         0.         0.         0.         0.30808281\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.10824991\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "newvector = vectorizer.transform(test_files)\n",
    "print(newvector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
